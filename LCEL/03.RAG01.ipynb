{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6a90a6-e296-4060-9e0e-34266f8c07e0",
   "metadata": {},
   "source": [
    "### Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9250a616-aa1d-48d4-ac4b-47c2226c1626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.350)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.352-py3-none-any.whl (794 kB)\n",
      "Requirement already satisfied: openai in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.9)\n",
      "Collecting openai\n",
      "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.4.21-py3-none-any.whl (508 kB)\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.0.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (1.26.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.1.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.0.70)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.9.0)\n",
      "Collecting posthog>=2.4.0\n",
      "  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
      "Collecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting fastapi>=0.95.2\n",
      "  Downloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n",
      "Collecting typer>=0.9.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-win_amd64.whl (158 kB)\n",
      "Collecting tokenizers>=0.13.2\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.16.3-cp310-cp310-win_amd64.whl (7.4 MB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
      "Collecting pulsar-client>=3.1.0\n",
      "  Downloading pulsar_client-3.3.0-cp310-cp310-win_amd64.whl (3.4 MB)\n",
      "Collecting chroma-hnswlib==0.7.3\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl (150 kB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Downloading mmh3-4.0.1-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Collecting starlette<0.33.0,>=0.29.0\n",
      "  Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Downloading google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Collecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting urllib3<2.0,>=1.24.2\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting importlib-metadata<7.0,>=6.0\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Collecting googleapis-common-protos~=1.52\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Collecting opentelemetry-proto==1.22.0\n",
      "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
      "Collecting backoff<3.0.0,>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-instrumentation==0.43b0\n",
      "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
      "Collecting opentelemetry-util-http==0.43b0\n",
      "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.43b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.43b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (57.4.0)\n",
      "Collecting asgiref~=3.0\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Downloading types_requests-2.31.0.9-py3-none-any.whl (14 kB)\n",
      "  Using cached types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n",
      "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
      "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
      "Collecting types-urllib3\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-0.21.0-cp310-none-win_amd64.whl (279 kB)\n",
      "Collecting python-dotenv>=0.13\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-12.0-cp310-cp310-win_amd64.whl (124 kB)\n",
      "Collecting httptools>=0.5.0\n",
      "  Downloading httptools-0.6.1-cp310-cp310-win_amd64.whl (58 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using legacy 'setup.py install' for bs4, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (PEP 517): started\n",
      "  Building wheel for pypika (PEP 517): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53836 sha256=0d231f85eead734d79e4d8ea30f289223a072b5d625f7bff7babaed7eb6470b5\n",
      "  Stored in directory: c:\\users\\owner\\appdata\\local\\pip\\cache\\wheels\\e1\\26\\51\\d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: zipp, wrapt, urllib3, importlib-metadata, deprecated, pyreadline3, pyasn1, protobuf, opentelemetry-api, rsa, pyasn1-modules, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, opentelemetry-instrumentation, oauthlib, mpmath, humanfriendly, fsspec, filelock, click, cachetools, backoff, asgiref, websockets, watchfiles, uvicorn, types-urllib3, sympy, starlette, requests-oauthlib, python-dotenv, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-common, monotonic, huggingface-hub, httptools, grpcio, googleapis-common-protos, google-auth, flatbuffers, coloredlogs, types-requests, typer, tokenizers, pypika, pulsar-client, posthog, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, mmh3, kubernetes, importlib-resources, fastapi, chroma-hnswlib, bcrypt, openai, langchainhub, langchain, chromadb, bs4\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.3.9\n",
      "    Uninstalling openai-1.3.9:\n",
      "      Successfully uninstalled openai-1.3.9\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.350\n",
      "    Uninstalling langchain-0.0.350:\n",
      "      Successfully uninstalled langchain-0.0.350\n",
      "    Running setup.py install for bs4: started\n",
      "    Running setup.py install for bs4: finished with status 'done'\n",
      "Successfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.1 cachetools-5.3.2 chroma-hnswlib-0.7.3 chromadb-0.4.21 click-8.1.7 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.108.0 filelock-3.13.1 flatbuffers-23.5.26 fsspec-2023.12.2 google-auth-2.25.2 googleapis-common-protos-1.62.0 grpcio-1.60.0 httptools-0.6.1 huggingface-hub-0.20.1 humanfriendly-10.0 importlib-metadata-6.11.0 importlib-resources-6.1.1 kubernetes-28.1.0 langchain-0.0.352 langchainhub-0.1.14 mmh3-4.0.1 monotonic-1.6 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.16.3 openai-1.6.1 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 posthog-3.1.0 protobuf-4.25.1 pulsar-client-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pypika-0.48.9 pyreadline3-3.4.1 python-dotenv-1.0.0 requests-oauthlib-1.3.1 rsa-4.9 starlette-0.32.0.post1 sympy-1.12 tokenizers-0.15.0 typer-0.9.0 types-requests-2.31.0.6 types-urllib3-1.26.25.14 urllib3-1.26.18 uvicorn-0.25.0 watchfiles-0.21.0 websockets-12.0 wrapt-1.16.0 zipp-3.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain openai chromadb langchainhub bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70111fc3-4651-49be-9570-9861a0fa83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c5c6c2-931b-447e-946e-eaa68903e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1beeaaa6-45b0-4f28-a78c-b5c0377dc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147157f6-19fd-4520-8897-76f8b9525f73",
   "metadata": {},
   "source": [
    "## Step1: Load - load the web data into as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7506e753-824d-4ad4-ae30-03560597c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dfb6e40-9822-48ad-8422-48aa152e6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader( \n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    },\n",
    ")\n",
    "docs=loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e805e88-e196-4f55-b407-3a564822d50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42824"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5a904d-d875-4bae-bbaf-cdf0709e3af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138455d-a70f-471c-90c2-3e2cdb2b3a0c",
   "metadata": {},
   "source": [
    "## step2 - Split - split each document into smaller chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e61c35-2954-4065-b460-f7a09bacffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcb752a0-9275-42e9-9445-837da6267374",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "all_splits=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e108337-89b8-4f25-a597-a1de46294c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fdb46aa-179a-4010-8706-f8cbfc97afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a0ddad3-4b6d-47e4-93e4-09a41c609f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7470}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[11].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa18d1-3830-4f2a-8147-2d330e513f39",
   "metadata": {},
   "source": [
    "## step3: Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ca57b41-9afb-4546-a4a1-f1c648eccc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd7ad144-4564-458e-8416-38152ea2f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e48f4-ab0b-4af6-9f87-431b8a432fc2",
   "metadata": {},
   "source": [
    "## step4: Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da91061-791c-4dd0-9199-9b1b582fb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03f96703-6470-4006-829c-04fe73487102",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\n",
    "    \"What are the approaches to Task Decomposition?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acbafdaa-c572-463d-8710-967d502d1fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connection error caused failure to post http://localhost:1984/runs  in LangSmith API. Please confirm your LANGCHAIN_ENDPOINT. ConnectionError(MaxRetryError(\"HTTPConnectionPool(host='localhost', port=1984): Max retries exceeded with url: /runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002122E399300>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\"))\n",
      "Connection error caused failure to patch http://localhost:1984/runs/4994342b-8926-4465-8afe-deb956037dd1  in LangSmith API. Please confirm your LANGCHAIN_ENDPOINT. ConnectionError(MaxRetryError(\"HTTPConnectionPool(host='localhost', port=1984): Max retries exceeded with url: /runs/4994342b-8926-4465-8afe-deb956037dd1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002122E3992D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\"))\n"
     ]
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edd8bc50-3a4a-4b11-aee9-e1848ad643c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb52536-c005-4217-b29a-99a6e2f770ca",
   "metadata": {},
   "source": [
    "## step5: Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2388b55b-10d5-4c2f-85d3-13c39876e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aadef17b-fdaa-464e-b5b3-3f022e4ebc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34914b78-7326-4431-8106-47a9057b1975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt.invoke(\n",
    "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96d98a9e-a232-4c94-87b8-4abec7da43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbe0143b-30fd-4526-a726-4e56c7f46fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for easier interpretation and execution by autonomous agents or models. Task decomposition can be done through various methods, such as using prompting techniques, task-specific instructions, or human inputs."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de45a2-f764-43fb-91f0-2074160a1714",
   "metadata": {},
   "source": [
    "## Customizing the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2746a64f-35dd-4754-a161-ea3c7aa2162e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63d1bb-a7f5-4d13-92ab-38668caaaba6",
   "metadata": {},
   "source": [
    "## Adding sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "352abe05-5721-4bf7-8ff5-1e2b85b72b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 1585},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 2192},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 17804},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 17414},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 29630},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 19373}],\n",
       " 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "rag_chain_with_source.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b84c3-bfb1-42e9-94f2-fa0bef8a2d34",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "543b057c-9847-4620-b77b-3b03401e1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "condense_q_chain = condense_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "598b11a2-881e-4eb0-b575-e7ef6f2fb682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"large\" in the context of a language model?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5d300e5-7005-4884-be6a-06501ed51845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do transformer models function?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"How do transformers work\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f815524-4a68-4b74-b69a-22277fa8c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def condense_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return condense_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84a59dbd-053b-479c-8ce3-ec8beda3c14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. This approach utilizes more computation at test-time and sheds light on the model\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with specific instructions, such as asking for the steps or subgoals for achieving a particular task. This simple prompting technique helps in breaking down the task into manageable components.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, when writing a novel, the instruction \"Write a story outline\" can be given to help the model break down the task into smaller writing tasks.\\n\\n4. Human inputs: In some cases, human inputs can be used for task decomposition. Humans can provide their expertise and knowledge to identify the steps or subtasks involved in a complex task, which can then be used to guide the model\\'s decomposition process.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d228ef-c692-4d0f-8079-24164304ec67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
